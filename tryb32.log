WARNING:tensorflow:Not all hyperparameter sets work on TPU. Prefer hparams_sets with a '_tpu' suffix, e.g. transformer_tpu, if available for your model.
INFO:tensorflow:Overriding hparams in mtf_transformer_paper_tr_0_mesh_8 with mesh_shape=batch:32
WARNING:tensorflow:Estimator's model_fn (<function wrapping_model_fn at 0x7f5ffb19b7d0>) includes params argument, but params are not passed to Estimator.
INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_num_ps_replicas': 0, '_keep_checkpoint_max': 20, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=8, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5ffaf3c1d0>, '_model_dir': 'gs://ssystore1/anothertransformer2019-05-06-07:54:49/out', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': gpu_options {
  per_process_gpu_memory_fraction: 0.95
}
allow_soft_placement: true
graph_options {
}
cluster_def {
  job {
    name: "worker"
    tasks {
      key: 0
      value: "10.240.1.4:8470"
    }
    tasks {
      key: 1
      value: "10.240.1.3:8470"
    }
    tasks {
      key: 2
      value: "10.240.1.2:8470"
    }
    tasks {
      key: 3
      value: "10.240.1.5:8470"
    }
  }
}
isolate_session_state: true
, 'use_tpu': True, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f5ffb884550>, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': None, '_evaluation_master': u'grpc://10.240.1.4:8470', '_eval_distribute': None, '_train_distribute': None, '_master': u'grpc://10.240.1.4:8470'}
INFO:tensorflow:_TPUContext: eval_on_tpu True
WARNING:tensorflow:ValidationMonitor only works with --schedule=train_and_evaluate
INFO:tensorflow:Not using Distribute Coordinator.
WARNING:tensorflow:TF_CONFIG should not be empty in distributed environment.
INFO:tensorflow:Start Tensorflow server.
2019-05-06 07:54:56.049987: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-05-06 07:54:56.055092: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2019-05-06 07:54:56.055394: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55ef67059340 executing computations on platform Host. Devices:
2019-05-06 07:54:56.055444: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-05-06 07:54:56.056833: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:252] Initialize GrpcChannelCache for job worker -> {0 -> localhost:8470, 1 -> 10.240.1.3:8470, 2 -> 10.240.1.2:8470, 3 -> 10.240.1.5:8470}
2019-05-06 07:54:56.057971: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:391] Started server with target: grpc://localhost:8470
INFO:tensorflow:Waiting 5 secs before starting training.
INFO:tensorflow:Querying Tensorflow master (grpc://10.240.1.4:8470) for TPU system metadata.
2019-05-06 07:55:01.194217: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:354] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.
INFO:tensorflow:Initializing TPU system (master: grpc://10.240.1.4:8470) to fetch topology for model parallelism. This might take a while.
INFO:tensorflow:Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 32
INFO:tensorflow:*** Num TPU Workers: 4
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 11905762922702603006)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 15396312354446413993)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 14826597956799940417)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 10343074958554233240)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 13739193233105508252)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5933165715177447641)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 5486537889288380392)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 15239902251989780788)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 11899970522679433567)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 1933800306117303804)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 12452358895163760288)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:CPU:0, CPU, -1, 17003106345392444542)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:XLA_CPU:0, XLA_CPU, 17179869184, 5868411300355850516)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:0, TPU, 17179869184, 1757086385049784139)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:1, TPU, 17179869184, 4967969961518831719)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:2, TPU, 17179869184, 15368016939966764969)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:3, TPU, 17179869184, 7727681261876670532)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:4, TPU, 17179869184, 18445124023743700623)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:5, TPU, 17179869184, 2731886874289222603)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:6, TPU, 17179869184, 13661923094110830229)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU:7, TPU, 17179869184, 7783455039473160492)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:1/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 16547143959398554392)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:CPU:0, CPU, -1, 3984505655874662011)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:XLA_CPU:0, XLA_CPU, 17179869184, 17816717722198902466)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:0, TPU, 17179869184, 17524576203735715590)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:1, TPU, 17179869184, 16500817508913748765)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:2, TPU, 17179869184, 11150880177615400804)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:3, TPU, 17179869184, 10763289613937276231)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:4, TPU, 17179869184, 89974670916145699)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:5, TPU, 17179869184, 18276413759941173271)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:6, TPU, 17179869184, 16670260518594707787)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU:7, TPU, 17179869184, 3423715698773488783)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:2/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 382375592525229490)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:CPU:0, CPU, -1, 12754541134919624165)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:XLA_CPU:0, XLA_CPU, 17179869184, 4552884070529256321)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:0, TPU, 17179869184, 1791608456752402163)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:1, TPU, 17179869184, 2705409532092469843)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:2, TPU, 17179869184, 100826654770723490)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:3, TPU, 17179869184, 6374260896116479921)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:4, TPU, 17179869184, 17744843166590081452)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:5, TPU, 17179869184, 4783618480876818500)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:6, TPU, 17179869184, 3061088728846026285)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU:7, TPU, 17179869184, 3874195012085048730)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:3/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 5887642898973621400)
INFO:tensorflow:Error recorded from training_loop: TPUConfig.num_shards is not set correctly. According to TPU system metadata for Tensorflow master (grpc://10.240.1.4:8470): num_replicas should be (32), got (8). For non-model-parallelism, num_replicas should be the total num of TPU cores in the system. For model-parallelism, the total number of TPU cores should be num_cores_per_replica * num_replicas. Please set it accordingly or leave it as `None`
INFO:tensorflow:training_loop marked as finished
WARNING:tensorflow:Reraising captured error
Traceback (most recent call last):
  File "/home/shengyushen_gmail_com/.local/bin/t2t-trainer", line 33, in <module>
    tf.app.run()
  File "/home/shengyushen_gmail_com/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py", line 125, in run
    _sys.exit(main(argv))
  File "/home/shengyushen_gmail_com/.local/bin/t2t-trainer", line 28, in main
    t2t_trainer.main(argv)
  File "/home/shengyushen_gmail_com/.local/lib/python2.7/site-packages/tensor2tensor/bin/t2t_trainer.py", line 401, in main
    execute_schedule(exp)
  File "/home/shengyushen_gmail_com/.local/lib/python2.7/site-packages/tensor2tensor/bin/t2t_trainer.py", line 356, in execute_schedule
    getattr(exp, FLAGS.schedule)()
  File "/home/shengyushen_gmail_com/.local/lib/python2.7/site-packages/tensor2tensor/utils/trainer_lib.py", line 401, in continuous_train_and_eval
    self._eval_spec)
  File "/home/shengyushen_gmail_com/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/training.py", line 471, in train_and_evaluate
    return executor.run()
  File "/home/shengyushen_gmail_com/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/training.py", line 638, in run
    getattr(self, task_to_run)()
  File "/home/shengyushen_gmail_com/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/training.py", line 648, in run_worker
    return self._start_distributed_training()
  File "/home/shengyushen_gmail_com/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/training.py", line 789, in _start_distributed_training
    saving_listeners=saving_listeners)
  File "/home/shengyushen_gmail_com/.local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py", line 2457, in train
    rendezvous.raise_errors()
  File "/home/shengyushen_gmail_com/.local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/error_handling.py", line 128, in raise_errors
    six.reraise(typ, value, traceback)
  File "/home/shengyushen_gmail_com/.local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py", line 2452, in train
    saving_listeners=saving_listeners)
  File "/home/shengyushen_gmail_com/.local/lib/python2.7/site-packages/tensorflow_estimator/python/estimator/estimator.py", line 355, in train
    hooks.extend(self._convert_train_steps_to_hooks(steps, max_steps))
  File "/home/shengyushen_gmail_com/.local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py", line 2328, in _convert_train_steps_to_hooks
    if ctx.is_running_on_cpu():
  File "/home/shengyushen_gmail_com/.local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_context.py", line 400, in is_running_on_cpu
    self._validate_tpu_configuration()
  File "/home/shengyushen_gmail_com/.local/lib/python2.7/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_context.py", line 602, in _validate_tpu_configuration
    raise ValueError(message)
ValueError: TPUConfig.num_shards is not set correctly. According to TPU system metadata for Tensorflow master (grpc://10.240.1.4:8470): num_replicas should be (32), got (8). For non-model-parallelism, num_replicas should be the total num of TPU cores in the system. For model-parallelism, the total number of TPU cores should be num_cores_per_replica * num_replicas. Please set it accordingly or leave it as `None`
